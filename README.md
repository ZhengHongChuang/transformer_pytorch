# A Pytorch Implementation of [Transformer](https://arxiv.org/pdf/1706.03762)

## Model Architecture
![Model Architecture](image/model.png)
### Positional Encoding
![Positional Encoding](image/positional_encoding.png)
### Scaled Dot-Product Attention
![Scaled Dot-Product Attention](image/scale_dot_product_attention.jpg)
### Multi-Head Attention
![Multi-Head Attention](image/multi_head_attention.jpg)
### Position-wise Feed-Forward Networks
![Position-wise Feed-Forward Networks](image/positionwise_feed_forward.jpg)
### Layer Norm
![Layer Norm](image/layer_norm.jpg)
## Requirements
* Python == 3.8.9
* pytorch == 2.3.0+cu121
* tqdm == 4.66.2
